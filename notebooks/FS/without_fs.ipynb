{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "204615e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88638c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\SML_Projects\\SML_airplane_price_project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b789879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/preprocessed/preprocessed_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2875f585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12377 entries, 0 to 12376\n",
      "Data columns (total 11 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   Model                        12377 non-null  float64\n",
      " 1   Year_of_Manufacture          12377 non-null  float64\n",
      " 2   Number_of_Engines            12377 non-null  float64\n",
      " 3   Engine_Type                  12377 non-null  float64\n",
      " 4   Capacity                     12377 non-null  float64\n",
      " 5   Range_(km)                   12377 non-null  int64  \n",
      " 6   Fuel_Consumption_(L/hour)    12377 non-null  float64\n",
      " 7   Hourly_Maintenance_Cost_($)  12377 non-null  float64\n",
      " 8   Age                          12377 non-null  float64\n",
      " 9   Sales_Region                 12377 non-null  float64\n",
      " 10  Price_($)                    12377 non-null  float64\n",
      "dtypes: float64(10), int64(1)\n",
      "memory usage: 1.0 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b15257a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.sample(frac=0.1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d82a9c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12377 entries, 0 to 12376\n",
      "Data columns (total 11 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   Model                        12377 non-null  float64\n",
      " 1   Year_of_Manufacture          12377 non-null  float64\n",
      " 2   Number_of_Engines            12377 non-null  float64\n",
      " 3   Engine_Type                  12377 non-null  float64\n",
      " 4   Capacity                     12377 non-null  float64\n",
      " 5   Range_(km)                   12377 non-null  int64  \n",
      " 6   Fuel_Consumption_(L/hour)    12377 non-null  float64\n",
      " 7   Hourly_Maintenance_Cost_($)  12377 non-null  float64\n",
      " 8   Age                          12377 non-null  float64\n",
      " 9   Sales_Region                 12377 non-null  float64\n",
      " 10  Price_($)                    12377 non-null  float64\n",
      "dtypes: float64(10), int64(1)\n",
      "memory usage: 1.0 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d360f529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a6ac3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop('Range_(km)', axis=1)   \n",
    "y = df['Range_(km)'] \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b1309b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc2a60e",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c64d176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression score: 0.9898800439745797\n",
      "Linear Regression MAE: 443.834075663444\n",
      "K-Fold mean: 0.9899142794140805\n",
      "K-Fold std: 0.00014702654980397416\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "lr.fit(x_train, y_train)\n",
    "y_pred = lr.predict(x_test)\n",
    "\n",
    "lr_score = r2_score(y_test, y_pred)\n",
    "lr_mae = mean_absolute_error(y_test, y_pred)\n",
    "lr_scores = cross_val_score(lr, x, y, cv=kf, scoring='r2')\n",
    "\n",
    "print(f'Linear Regression score: {lr_score}')\n",
    "print(f'Linear Regression MAE: {lr_mae}')\n",
    "print(\"K-Fold mean:\", lr_scores.mean())\n",
    "print(\"K-Fold std:\", lr_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc89b28",
   "metadata": {},
   "source": [
    "# Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f5f03fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso score: 0.9898800461128472\n",
      "Lasso MAE: 443.8342366345642\n",
      "K-Fold mean: 0.9899142795092627\n",
      "K-Fold std: 0.00014702454313368575\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha=0.0001)\n",
    "\n",
    "lasso.fit(x_train, y_train)\n",
    "y_pred = lasso.predict(x_test)\n",
    "\n",
    "lasso_score = r2_score(y_test, y_pred)\n",
    "lasso_mae = mean_absolute_error(y_test, y_pred)\n",
    "lasso_scores = cross_val_score(lasso, x, y, cv=kf, scoring='r2')\n",
    "\n",
    "print(f'Lasso score: {lasso_score}')\n",
    "print(f'Lasso MAE: {lasso_mae}')\n",
    "print(\"K-Fold mean:\", lasso_scores.mean())\n",
    "print(\"K-Fold std:\", lasso_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093367e4",
   "metadata": {},
   "source": [
    "# Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "012110d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge score: 0.9898793307767362\n",
      "Ridge MAE: 443.72993172339903\n",
      "K-Fold mean: 0.9899142232590573\n",
      "K-Fold std: 0.0001478841162568019\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge(alpha=0.1)\n",
    "\n",
    "ridge.fit(x_train, y_train)\n",
    "y_pred = ridge.predict(x_test)\n",
    "\n",
    "ridge_score = r2_score(y_test, y_pred)\n",
    "ridge_mae = mean_absolute_error(y_test, y_pred)\n",
    "ridge_scores = cross_val_score(ridge, x, y, cv=kf, scoring='r2')\n",
    "\n",
    "print(f'Ridge score: {ridge_score}')\n",
    "print(f'Ridge MAE: {ridge_mae}')\n",
    "print(\"K-Fold mean:\", ridge_scores.mean())\n",
    "print(\"K-Fold std:\", ridge_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2596ac12",
   "metadata": {},
   "source": [
    "# ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3406938",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.337e+09, tolerance: 2.982e+07\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.096e+09, tolerance: 2.498e+07\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.157e+09, tolerance: 2.481e+07\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet score: 0.9898712844209032\n",
      "ElasticNet MAE: 442.9286570550076\n",
      "K-Fold mean: 0.9899119503963426\n",
      "K-Fold std: 0.00015397747582300504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.164e+09, tolerance: 2.488e+07\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "en = ElasticNet(alpha=0.001, l1_ratio=0.9)\n",
    "\n",
    "en.fit(x_train, y_train)\n",
    "y_pred = en.predict(x_test)\n",
    "\n",
    "elastic_score = r2_score(y_test, y_pred)\n",
    "elastic_mae = mean_absolute_error(y_test, y_pred)\n",
    "elastic_scores = cross_val_score(en, x, y, cv=kf, scoring='r2')\n",
    "\n",
    "print(f'ElasticNet score: {elastic_score}')\n",
    "print(f'ElasticNet MAE: {elastic_mae}')\n",
    "print(\"K-Fold mean:\", elastic_scores.mean())\n",
    "print(\"K-Fold std:\", elastic_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbaa39f",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46db29a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree score: 1.0\n",
      "Decision Tree MAE: 0.0\n",
      "K-Fold mean: 1.0\n",
      "K-Fold std: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dt = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "dt.fit(x_train, y_train)\n",
    "y_pred = dt.predict(x_test)\n",
    "\n",
    "dt_score = r2_score(y_test, y_pred)\n",
    "dt_mae = mean_absolute_error(y_test, y_pred)\n",
    "dt_scores = cross_val_score(dt, x, y, cv=kf, scoring='r2')\n",
    "\n",
    "print(f'Decision Tree score: {dt_score}')\n",
    "print(f'Decision Tree MAE: {dt_mae}')\n",
    "print(\"K-Fold mean:\", dt_scores.mean())\n",
    "print(\"K-Fold std:\", dt_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a4bfd6",
   "metadata": {},
   "source": [
    "# RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0dc6ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest score: 1.0\n",
      "Random Forest MAE: 0.0\n",
      "K-Fold mean: 1.0\n",
      "K-Fold std: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "rf.fit(x_train, y_train)\n",
    "y_pred = rf.predict(x_test)\n",
    "\n",
    "rf_score = r2_score(y_test, y_pred)\n",
    "rf_mae = mean_absolute_error(y_test, y_pred)\n",
    "rf_scores = cross_val_score(rf, x, y, cv=kf, scoring='r2')\n",
    "\n",
    "print(f'Random Forest score: {rf_score}')\n",
    "print(f'Random Forest MAE: {rf_mae}')\n",
    "print(\"K-Fold mean:\", rf_scores.mean())\n",
    "print(\"K-Fold std:\", rf_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f178f7",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd27d43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting score: 0.9999999992943458\n",
      "Gradient Boosting MAE: 0.13183686008223522\n",
      "K-Fold mean: 0.9999999992944417\n",
      "K-Fold std: 3.5424756233243284e-14\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gb = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "gb.fit(x_train, y_train)\n",
    "y_pred = gb.predict(x_test)\n",
    "\n",
    "gbr_score = r2_score(y_test, y_pred)\n",
    "gbr_mae = mean_absolute_error(y_test, y_pred)\n",
    "gbr_scores = cross_val_score(gb, x, y, cv=kf, scoring='r2')\n",
    "\n",
    "print(f'Gradient Boosting score: {gbr_score}')\n",
    "print(f'Gradient Boosting MAE: {gbr_mae}')\n",
    "print(\"K-Fold mean:\", gbr_scores.mean())\n",
    "print(\"K-Fold std:\", gbr_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a8f562",
   "metadata": {},
   "source": [
    "# Extra Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6794aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Trees score: 0.9999999952340723\n",
      "Extra Trees MAE: 0.010157512116316677\n",
      "K-Fold mean: 0.9999999294873346\n",
      "K-Fold std: 5.433064904716884e-08\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "et = ExtraTreesRegressor(random_state=42)\n",
    "\n",
    "et.fit(x_train, y_train)\n",
    "y_pred = et.predict(x_test)\n",
    "\n",
    "et_score = r2_score(y_test, y_pred)\n",
    "et_mae = mean_absolute_error(y_test, y_pred)\n",
    "et_scores = cross_val_score(et, x, y, cv=kf, scoring='r2')\n",
    "\n",
    "print(f'Extra Trees score: {et_score}')\n",
    "print(f'Extra Trees MAE: {et_mae}')\n",
    "print(\"K-Fold mean:\", et_scores.mean())\n",
    "print(\"K-Fold std:\", et_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0569b03",
   "metadata": {},
   "source": [
    "# Hist Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3dd510ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hist Gradient Boosting score: 0.9999999992943458\n",
      "Hist Gradient Boosting MAE: 0.1318368607055448\n",
      "K-Fold mean: 0.9999999992944417\n",
      "K-Fold std: 3.5424756233243284e-14\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "hgb = HistGradientBoostingRegressor(random_state=42)\n",
    "\n",
    "hgb.fit(x_train, y_train)\n",
    "y_pred = hgb.predict(x_test)\n",
    "\n",
    "hgb_score = r2_score(y_test, y_pred)\n",
    "hgb_mae = mean_absolute_error(y_test, y_pred)\n",
    "hgb_scores = cross_val_score(hgb, x, y, cv=kf, scoring='r2')\n",
    "\n",
    "print(f'Hist Gradient Boosting score: {hgb_score}')\n",
    "print(f'Hist Gradient Boosting MAE: {hgb_mae}')\n",
    "print(\"K-Fold mean:\", hgb_scores.mean())\n",
    "print(\"K-Fold std:\", hgb_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41066085",
   "metadata": {},
   "source": [
    "# SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cadad758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR score: 0.9661621988944264\n",
      "SVR MAE: 621.3250019327177\n",
      "K-Fold mean: 0.950791560793098\n",
      "K-Fold std: 0.0011270901221601497\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "svr = SVR(kernel='rbf', C=10.0)\n",
    "\n",
    "svr.fit(x_train, y_train)\n",
    "y_pred = svr.predict(x_test)\n",
    "\n",
    "svr_score = r2_score(y_test, y_pred)\n",
    "svr_mae = mean_absolute_error(y_test, y_pred)\n",
    "svr_scores = cross_val_score(svr, x, y, cv=kf, scoring='r2')\n",
    "\n",
    "print(f'SVR score: {svr_score}')\n",
    "print(f'SVR MAE: {svr_mae}')\n",
    "print(\"K-Fold mean:\", svr_scores.mean())\n",
    "print(\"K-Fold std:\", svr_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5553bb",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64510482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN score: 1.0\n",
      "KNN MAE: 0.0\n",
      "K-Fold mean: 1.0\n",
      "K-Fold std: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=1)\n",
    "\n",
    "knn.fit(x_train, y_train)\n",
    "y_pred = knn.predict(x_test)\n",
    "\n",
    "knn_score = r2_score(y_test, y_pred)\n",
    "knn_mae = mean_absolute_error(y_test, y_pred)\n",
    "knn_scores = cross_val_score(knn, x, y, cv=kf, scoring='r2')\n",
    "\n",
    "print(f'KNN score: {knn_score}')\n",
    "print(f'KNN MAE: {knn_mae}')\n",
    "print(\"K-Fold mean:\", knn_scores.mean())\n",
    "print(\"K-Fold std:\", knn_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fae645b",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c3989ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost score: 1.0\n",
      "XGBoost MAE: 0.0005456188227981329\n",
      "K-Fold mean: 1.0\n",
      "K-Fold std: 0.0\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "xgb_model = XGBRegressor()\n",
    "\n",
    "xgb_model.fit(x_train, y_train)\n",
    "y_pred = xgb_model.predict(x_test)\n",
    "\n",
    "xgb_score = r2_score(y_test, y_pred)\n",
    "xgb_mae = mean_absolute_error(y_test, y_pred)\n",
    "xgb_scores = cross_val_score(xgb_model, x, y, cv=kf, scoring='r2')\n",
    "\n",
    "print(f'XGBoost score: {xgb_score}')\n",
    "print(f'XGBoost MAE: {xgb_mae}')\n",
    "print(\"K-Fold mean:\", xgb_scores.mean())\n",
    "print(\"K-Fold std:\", xgb_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06239c4e",
   "metadata": {},
   "source": [
    "# Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d9a1e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost score: 1.0\n",
      "AdaBoost MAE: 0.0\n",
      "K-Fold mean: 1.0\n",
      "K-Fold std: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "ab = AdaBoostRegressor()\n",
    "\n",
    "ab.fit(x_train, y_train)\n",
    "y_pred = ab.predict(x_test)\n",
    "\n",
    "ab_score = r2_score(y_test, y_pred)\n",
    "ab_mae = mean_absolute_error(y_test, y_pred)\n",
    "ab_scores = cross_val_score(ab, x, y, cv=kf, scoring='r2')\n",
    "\n",
    "print(f'AdaBoost score: {ab_score}')\n",
    "print(f'AdaBoost MAE: {ab_mae}')\n",
    "print(\"K-Fold mean:\", ab_scores.mean())\n",
    "print(\"K-Fold std:\", ab_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c035ca",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6ea8cedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000647 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 875\n",
      "[LightGBM] [Info] Number of data points in the train set: 9901, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 7798.079992\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000498 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 875\n",
      "[LightGBM] [Info] Number of data points in the train set: 8251, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 7792.443340\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000517 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 875\n",
      "[LightGBM] [Info] Number of data points in the train set: 8251, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 7760.310265\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000323 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 875\n",
      "[LightGBM] [Info] Number of data points in the train set: 8252, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 7793.875424\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "LGBM Regressor R2: 0.9999999992943458\n",
      "LGBM Regressor MAE: 0.1318368607055448\n",
      "K-Fold mean: 0.9999999992944417\n",
      "K-Fold std: 3.5424756233243284e-14\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "lgbm = LGBMRegressor(random_state=42)\n",
    "\n",
    "lgbm.fit(x_train, y_train)\n",
    "y_pred = lgbm.predict(x_test)\n",
    "\n",
    "lgbm_score = r2_score(y_test, y_pred)\n",
    "lgbm_mae = mean_absolute_error(y_test, y_pred)\n",
    "lgbm_scores = cross_val_score(lgbm, x, y, cv=kf, scoring='r2')\n",
    "\n",
    "print(\"LGBM Regressor R2:\", lgbm_score)\n",
    "print(\"LGBM Regressor MAE:\", lgbm_mae)\n",
    "print(\"K-Fold mean:\", lgbm_scores.mean())\n",
    "print(\"K-Fold std:\", lgbm_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582711df",
   "metadata": {},
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "515e74d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Regressor R2: 1.0\n",
      "Bagging Regressor MAE: 0.0\n",
      "K-Fold mean: 1.0\n",
      "K-Fold std: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "bag = BaggingRegressor(random_state=42)\n",
    "\n",
    "bag.fit(x_train, y_train)\n",
    "y_pred = bag.predict(x_test)\n",
    "\n",
    "bag_score = r2_score(y_test, y_pred)\n",
    "bag_mae = mean_absolute_error(y_test, y_pred)\n",
    "bag_scores = cross_val_score(bag, x, y, cv=kf, scoring='r2')\n",
    "\n",
    "print(\"Bagging Regressor R2:\", bag_score)\n",
    "print(\"Bagging Regressor MAE:\", bag_mae)\n",
    "print(\"K-Fold mean:\", bag_scores.mean())\n",
    "print(\"K-Fold std:\", bag_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39a657c",
   "metadata": {},
   "source": [
    "# Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0b1eb367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Regressor R2: 0.9988755414957099\n",
      "Voting Regressor MAE: 147.94807772518678\n",
      "K-Fold mean R2: 0.9988792339991326\n",
      "K-Fold std R2: 1.6454290808025073e-05\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "model1 = RandomForestRegressor(random_state=42)\n",
    "model2 = ExtraTreesRegressor(random_state=42)\n",
    "model3 = LinearRegression()\n",
    "\n",
    "voting = VotingRegressor(\n",
    "    estimators=[\n",
    "        ('rf', model1),\n",
    "        ('et', model2),\n",
    "        ('lr', model3)\n",
    "    ],\n",
    ")\n",
    "\n",
    "voting.fit(x_train, y_train)\n",
    "y_pred = voting.predict(x_test)\n",
    "\n",
    "voting_score = r2_score(y_test, y_pred)\n",
    "voting_mae = mean_absolute_error(y_test, y_pred)\n",
    "voting_scores = cross_val_score(voting, x, y, cv=kf, scoring='r2')\n",
    "\n",
    "print(\"Voting Regressor R2:\", voting_score)\n",
    "print(\"Voting Regressor MAE:\", voting_mae)\n",
    "print(\"K-Fold mean R2:\", voting_scores.mean())\n",
    "print(\"K-Fold std R2:\", voting_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d1400b",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ef98c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Regressor R2: 1.0\n",
      "Stacking Regressor MAE: 3.473041061347129e-11\n",
      "K-Fold mean R2: 1.0\n",
      "K-Fold std R2: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "base1 = RandomForestRegressor(random_state=42)\n",
    "base2 = ExtraTreesRegressor(random_state=42)\n",
    "base3 = LinearRegression()\n",
    "\n",
    "stacking = StackingRegressor(\n",
    "    estimators=[\n",
    "        ('rf', base1),\n",
    "        ('et', base2),\n",
    "        ('lr', base3)\n",
    "    ],\n",
    "    final_estimator=LinearRegression()\n",
    ")\n",
    "\n",
    "stacking.fit(x_train, y_train)\n",
    "y_pred = stacking.predict(x_test)\n",
    "\n",
    "stacking_score = r2_score(y_test, y_pred)\n",
    "stacking_mae = mean_absolute_error(y_test, y_pred)\n",
    "stacking_scores = cross_val_score(stacking, x, y,  cv=kf, scoring='r2')\n",
    "\n",
    "print(\"Stacking Regressor R2:\", stacking_score)\n",
    "print(\"Stacking Regressor MAE:\", stacking_mae)\n",
    "print(\"K-Fold mean R2:\", stacking_scores.mean())\n",
    "print(\"K-Fold std R2:\", stacking_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2d4496",
   "metadata": {},
   "source": [
    "# Bagged KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a21d2120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagged KNN Regressor R2: 1.0\n",
      "Bagged KNN Regressor MAE: 0.0\n",
      "K-Fold mean R2: 1.0\n",
      "K-Fold std R2: 0.0\n"
     ]
    }
   ],
   "source": [
    "bag_knn = BaggingRegressor(\n",
    "    estimator=KNeighborsRegressor(),\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "bag_knn.fit(x_train, y_train)\n",
    "y_pred = bag_knn.predict(x_test)\n",
    "\n",
    "bag_knn_score = r2_score(y_test, y_pred)\n",
    "bag_knn_mae = mean_absolute_error(y_test, y_pred)\n",
    "bag_knn_scores = cross_val_score(bag_knn, x_train, y_train, cv=kf, scoring='r2')\n",
    "\n",
    "print(\"Bagged KNN Regressor R2:\", bag_knn_score)\n",
    "print(\"Bagged KNN Regressor MAE:\", bag_knn_mae)\n",
    "print(\"K-Fold mean R2:\", bag_knn_scores.mean())\n",
    "print(\"K-Fold std R2:\", bag_knn_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a741a06d",
   "metadata": {},
   "source": [
    "# Bagged DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cb0c43e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagged DT Regressor R2: 1.0\n",
      "Bagged DT Regressor MAE: 0.0\n",
      "K-Fold mean R2: 1.0\n",
      "K-Fold std R2: 0.0\n"
     ]
    }
   ],
   "source": [
    "bag_dt = BaggingRegressor(\n",
    "    estimator=DecisionTreeRegressor(),\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "bag_dt.fit(x_train, y_train)\n",
    "y_pred = bag_dt.predict(x_test)\n",
    "\n",
    "bag_dt_score = r2_score(y_test, y_pred)\n",
    "bag_dt_mae = mean_absolute_error(y_test, y_pred)\n",
    "bag_dt_scores = cross_val_score(bag_dt, x_train, y_train, cv=kf, scoring='r2')\n",
    "\n",
    "print(\"Bagged DT Regressor R2:\", bag_dt_score)\n",
    "print(\"Bagged DT Regressor MAE:\", bag_dt_mae)\n",
    "print(\"K-Fold mean R2:\", bag_dt_scores.mean())\n",
    "print(\"K-Fold std R2:\", bag_dt_scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7dc00824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                Without FS Comparison                                 </span>\n",
       "\n",
       "<span style=\"font-weight: bold\"> Algorithm              </span><span style=\"font-weight: bold\"> R2 score </span><span style=\"font-weight: bold\"> Mean Absolute Error </span><span style=\"font-weight: bold\"> K-Fold mean </span><span style=\"font-weight: bold\"> K-Fold std </span>\n",
       "\n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Decision Tree</span>           <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.00</span>      <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.00</span>                 <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.00</span>         <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.00</span>       \n",
       "\n",
       " Random Forest           1.00      0.00                 1.00         0.00       \n",
       "\n",
       " KNN                     1.00      0.00                 1.00         0.00       \n",
       "\n",
       " XGBoost                 1.00      0.00                 1.00         0.00       \n",
       "\n",
       " AdaBoost                1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Bagging                 1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Stacking                1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Bagged KNN              1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Bagged DT               1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Gradient Boosting       1.00      0.13                 1.00         0.00       \n",
       "\n",
       " Hist Gradient Boosting  1.00      0.13                 1.00         0.00       \n",
       "\n",
       " LGBMRegressor           1.00      0.13                 1.00         0.00       \n",
       "\n",
       " Extra Trees             1.00      0.01                 1.00         0.00       \n",
       "\n",
       " Hard Voting             1.00      147.95               1.00         0.00       \n",
       "\n",
       " Lasso                   0.99      443.83               0.99         0.00       \n",
       "\n",
       " Linear Regression       0.99      443.83               0.99         0.00       \n",
       "\n",
       " Ridge                   0.99      443.73               0.99         0.00       \n",
       "\n",
       " ElasticNet              0.99      442.93               0.99         0.00       \n",
       "\n",
       " <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">SVR</span>                     <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.97</span>      <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">621.33</span>               <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.95</span>         <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.00</span>       \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                Without FS Comparison                                 \u001b[0m\n",
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mAlgorithm             \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mR2 score\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mMean Absolute Error\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mK-Fold mean\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mK-Fold std\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " \u001b[1;32mDecision Tree\u001b[0m           \u001b[1;32m1.00\u001b[0m      \u001b[1;32m0.00\u001b[0m                 \u001b[1;32m1.00\u001b[0m         \u001b[1;32m0.00\u001b[0m       \n",
       "\n",
       " Random Forest           1.00      0.00                 1.00         0.00       \n",
       "\n",
       " KNN                     1.00      0.00                 1.00         0.00       \n",
       "\n",
       " XGBoost                 1.00      0.00                 1.00         0.00       \n",
       "\n",
       " AdaBoost                1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Bagging                 1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Stacking                1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Bagged KNN              1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Bagged DT               1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Gradient Boosting       1.00      0.13                 1.00         0.00       \n",
       "\n",
       " Hist Gradient Boosting  1.00      0.13                 1.00         0.00       \n",
       "\n",
       " LGBMRegressor           1.00      0.13                 1.00         0.00       \n",
       "\n",
       " Extra Trees             1.00      0.01                 1.00         0.00       \n",
       "\n",
       " Hard Voting             1.00      147.95               1.00         0.00       \n",
       "\n",
       " Lasso                   0.99      443.83               0.99         0.00       \n",
       "\n",
       " Linear Regression       0.99      443.83               0.99         0.00       \n",
       "\n",
       " Ridge                   0.99      443.73               0.99         0.00       \n",
       "\n",
       " ElasticNet              0.99      442.93               0.99         0.00       \n",
       "\n",
       " \u001b[1;31mSVR\u001b[0m                     \u001b[1;31m0.97\u001b[0m      \u001b[1;31m621.33\u001b[0m               \u001b[1;31m0.95\u001b[0m         \u001b[1;31m0.00\u001b[0m       \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich.table import Table\n",
    "from rich.console import Console\n",
    "\n",
    "console = Console()\n",
    "\n",
    "results = [\n",
    "    ['Linear Regression', lr_score, lr_mae, lr_scores.mean(), lr_scores.std()],\n",
    "    ['Lasso', lasso_score, lasso_mae, lasso_scores.mean(), lasso_scores.std()],\n",
    "    ['Ridge', ridge_score, ridge_mae, ridge_scores.mean(), ridge_scores.std()],\n",
    "    ['ElasticNet', elastic_score, elastic_mae, elastic_scores.mean(), elastic_scores.std()],\n",
    "    ['Decision Tree', dt_score, dt_mae, dt_scores.mean(), dt_scores.std()],\n",
    "    ['Random Forest', rf_score, rf_mae, rf_scores.mean(), rf_scores.std()],\n",
    "    ['Gradient Boosting', gbr_score, gbr_mae, gbr_scores.mean(), gbr_scores.std()],\n",
    "    ['Extra Trees', et_score, et_mae, et_scores.mean(), et_scores.std()],\n",
    "    ['Hist Gradient Boosting', hgb_score, hgb_mae, hgb_scores.mean(), hgb_scores.std()],\n",
    "    ['SVR', svr_score, svr_mae, svr_scores.mean(), svr_scores.std()],\n",
    "    ['KNN', knn_score, knn_mae, knn_scores.mean(), knn_scores.std()],\n",
    "    ['XGBoost', xgb_score, xgb_mae, xgb_scores.mean(), xgb_scores.std()],\n",
    "    ['AdaBoost', ab_score, ab_mae, ab_scores.mean(), ab_scores.std()],\n",
    "    ['LGBMRegressor', lgbm_score, lgbm_mae, lgbm_scores.mean(), lgbm_scores.std()],\n",
    "    ['Bagging', bag_score, bag_mae, bag_scores.mean(), bag_scores.std()],\n",
    "    ['Hard Voting', voting_score, voting_mae, voting_scores.mean(), voting_scores.std()],\n",
    "    ['Stacking', stacking_score, stacking_mae, stacking_scores.mean(), stacking_scores.std()],\n",
    "    ['Bagged KNN', bag_knn_score, bag_knn_mae, bag_knn_scores.mean(), bag_knn_scores.std()],\n",
    "    ['Bagged DT', bag_dt_score, bag_dt_mae, bag_dt_scores.mean(), bag_dt_scores.std()],\n",
    "]\n",
    "\n",
    "result_sorted = sorted(results, key=lambda i: i[1], reverse=True)\n",
    "\n",
    "best_model = max(results, key=lambda x: x[1])\n",
    "worst_model = min(results, key=lambda x: x[1])\n",
    "\n",
    "table = Table(title=\"Without FS Comparison\", show_lines=True)\n",
    "table.add_column(\"Algorithm\")\n",
    "table.add_column(\"R2 score\")\n",
    "table.add_column(\"Mean Absolute Error\")\n",
    "table.add_column(\"K-Fold mean\")\n",
    "table.add_column(\"K-Fold std\")\n",
    "\n",
    "for row in result_sorted:\n",
    "    algo, r2, mae, kmean, kstd = row\n",
    "\n",
    "    if row == best_model:\n",
    "        table.add_row(\n",
    "            f\"[bold green]{algo}[/bold green]\",\n",
    "            f\"[bold green]{r2:.2f}[/bold green]\",\n",
    "            f\"[bold green]{mae:.2f}[/bold green]\",\n",
    "            f\"[bold green]{kmean:.2f}[/bold green]\",\n",
    "            f\"[bold green]{kstd:.2f}[/bold green]\",\n",
    "        )\n",
    "    elif row == worst_model:\n",
    "        table.add_row(\n",
    "            f\"[bold red]{algo}[/bold red]\",\n",
    "            f\"[bold red]{r2:.2f}[/bold red]\",\n",
    "            f\"[bold red]{mae:.2f}[/bold red]\",\n",
    "            f\"[bold red]{kmean:.2f}[/bold red]\",\n",
    "            f\"[bold red]{kstd:.2f}[/bold red]\",\n",
    "        )\n",
    "    else:\n",
    "        table.add_row(algo, f\"{r2:.2f}\", f\"{mae:.2f}\", f\"{kmean:.2f}\", f\"{kstd:.2f}\")\n",
    "\n",
    "console.print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b666337b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                Without FS Comparison                                 </span>\n",
       "\n",
       "<span style=\"font-weight: bold\"> Algorithm              </span><span style=\"font-weight: bold\"> R2 score </span><span style=\"font-weight: bold\"> Mean Absolute Error </span><span style=\"font-weight: bold\"> K-Fold mean </span><span style=\"font-weight: bold\"> K-Fold std </span>\n",
       "\n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Decision Tree</span>           <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.00</span>      <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.00</span>                 <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.00</span>         <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.00</span>       \n",
       "\n",
       " Random Forest           1.00      0.00                 1.00         0.00       \n",
       "\n",
       " KNN                     1.00      0.00                 1.00         0.00       \n",
       "\n",
       " XGBoost                 1.00      0.00                 1.00         0.00       \n",
       "\n",
       " AdaBoost                1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Bagging                 1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Stacking                1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Bagged KNN              1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Bagged DT               1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Gradient Boosting       1.00      0.13                 1.00         0.00       \n",
       "\n",
       " Hist Gradient Boosting  1.00      0.13                 1.00         0.00       \n",
       "\n",
       " LGBMRegressor           1.00      0.13                 1.00         0.00       \n",
       "\n",
       " Extra Trees             1.00      0.01                 1.00         0.00       \n",
       "\n",
       " Hard Voting             1.00      147.95               1.00         0.00       \n",
       "\n",
       " Lasso                   0.99      443.83               0.99         0.00       \n",
       "\n",
       " Linear Regression       0.99      443.83               0.99         0.00       \n",
       "\n",
       " Ridge                   0.99      443.73               0.99         0.00       \n",
       "\n",
       " ElasticNet              0.99      442.93               0.99         0.00       \n",
       "\n",
       " <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">SVR</span>                     <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.97</span>      <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">621.33</span>               <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.95</span>         <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.00</span>       \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                Without FS Comparison                                 \u001b[0m\n",
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mAlgorithm             \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mR2 score\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mMean Absolute Error\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mK-Fold mean\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mK-Fold std\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " \u001b[1;32mDecision Tree\u001b[0m           \u001b[1;32m1.00\u001b[0m      \u001b[1;32m0.00\u001b[0m                 \u001b[1;32m1.00\u001b[0m         \u001b[1;32m0.00\u001b[0m       \n",
       "\n",
       " Random Forest           1.00      0.00                 1.00         0.00       \n",
       "\n",
       " KNN                     1.00      0.00                 1.00         0.00       \n",
       "\n",
       " XGBoost                 1.00      0.00                 1.00         0.00       \n",
       "\n",
       " AdaBoost                1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Bagging                 1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Stacking                1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Bagged KNN              1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Bagged DT               1.00      0.00                 1.00         0.00       \n",
       "\n",
       " Gradient Boosting       1.00      0.13                 1.00         0.00       \n",
       "\n",
       " Hist Gradient Boosting  1.00      0.13                 1.00         0.00       \n",
       "\n",
       " LGBMRegressor           1.00      0.13                 1.00         0.00       \n",
       "\n",
       " Extra Trees             1.00      0.01                 1.00         0.00       \n",
       "\n",
       " Hard Voting             1.00      147.95               1.00         0.00       \n",
       "\n",
       " Lasso                   0.99      443.83               0.99         0.00       \n",
       "\n",
       " Linear Regression       0.99      443.83               0.99         0.00       \n",
       "\n",
       " Ridge                   0.99      443.73               0.99         0.00       \n",
       "\n",
       " ElasticNet              0.99      442.93               0.99         0.00       \n",
       "\n",
       " \u001b[1;31mSVR\u001b[0m                     \u001b[1;31m0.97\u001b[0m      \u001b[1;31m621.33\u001b[0m               \u001b[1;31m0.95\u001b[0m         \u001b[1;31m0.00\u001b[0m       \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "temp_console = Console(record=True)\n",
    "temp_console.print(table)\n",
    "text = temp_console.export_text()\n",
    "with open('results/feature_selection_compare.txt', 'a', encoding='utf-8') as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447421cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
